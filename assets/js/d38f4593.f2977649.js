"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[698],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return d}});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var c=a.createContext({}),l=function(e){var t=a.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},u=function(e){var t=l(e.components);return a.createElement(c.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,c=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),h=l(n),d=o,m=h["".concat(c,".").concat(d)]||h[d]||p[d]||i;return n?a.createElement(m,r(r({ref:t},u),{},{components:n})):a.createElement(m,r({ref:t},u))}));function d(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=h;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:o,r[1]=s;for(var l=2;l<i;l++)r[l]=n[l];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},2801:function(e,t,n){n.r(t),n.d(t,{assets:function(){return u},contentTitle:function(){return c},default:function(){return d},frontMatter:function(){return s},metadata:function(){return l},toc:function(){return p}});var a=n(7462),o=n(3366),i=(n(7294),n(3905)),r=["components"],s={id:"computation-caching",title:"Computation Caching"},c="Computation Caching",l={unversionedId:"core-concepts/computation-caching",id:"core-concepts/computation-caching",title:"Computation Caching",description:'When it comes to running tasks, caching etc., Lerna and Nx can be used interchangeably. When we say "Lerna can cache builds", we mean that Lerna uses Nx which can cache builds.',source:"@site/docs/core-concepts/caching.md",sourceDirName:"core-concepts",slug:"/core-concepts/computation-caching",permalink:"/docs/core-concepts/computation-caching",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/core-concepts/caching.md",tags:[],version:"current",frontMatter:{id:"computation-caching",title:"Computation Caching"},sidebar:"main",previous:{title:"Versioning and Publishing",permalink:"/docs/core-concepts/versioning-and-publishing"},next:{title:"Distributed Task Execution",permalink:"/docs/core-concepts/distributed-task-execution"}},u={},p=[{value:"Source Code Hash Inputs",id:"source-code-hash-inputs",level:2},{value:"Runtime Hash Inputs",id:"runtime-hash-inputs",level:2},{value:"Args Hash Inputs",id:"args-hash-inputs",level:2},{value:"What is Cached",id:"what-is-cached",level:2},{value:"Skipping Cache",id:"skipping-cache",level:2},{value:"Customizing the Cache Location",id:"customizing-the-cache-location",level:2},{value:"Local Computation Caching",id:"local-computation-caching",level:2},{value:"Distributed Computation Caching",id:"distributed-computation-caching",level:2}],h={toc:p};function d(e){var t=e.components,s=(0,o.Z)(e,r);return(0,i.kt)("wrapper",(0,a.Z)({},h,s,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"computation-caching"},"Computation Caching"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},'When it comes to running tasks, caching etc., Lerna and Nx can be used interchangeably. When we say "Lerna can cache builds", we mean that Lerna uses Nx which can cache builds.')),(0,i.kt)("p",null,"It's costly to rebuild and retest the same code over and over again. Lerna uses a computation cache to never rebuild the same code twice. This is how it does it."),(0,i.kt)("p",null,"Before running any task, Lerna computes its computation hash. As long as the computation hash is the same, the output of running the task is the same."),(0,i.kt)("p",null,"By default, the computation hash for - say - ",(0,i.kt)("inlineCode",{parentName:"p"},"lerna run test --scope=remixapp")," includes:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"All the source files of ",(0,i.kt)("inlineCode",{parentName:"li"},"remixapp")," and its dependencies"),(0,i.kt)("li",{parentName:"ul"},"Relevant global configuration"),(0,i.kt)("li",{parentName:"ul"},"Versions of external dependencies"),(0,i.kt)("li",{parentName:"ul"},"Runtime values provisioned by the user such as the version of Node"),(0,i.kt)("li",{parentName:"ul"},"CLI Command flags")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"computation-hashing",src:n(3238).Z,width:"2088",height:"1580"})),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"This behavior is customizable. For instance, lint checks may only depend on the source code of the project and global\nconfigs. Builds can depend on the dts files of the compiled libs instead of their source.")),(0,i.kt)("p",null,"After Lerna computes the hash for a task, it then checks if it ran this exact computation before. First, it checks locally, and then if it is missing, and if a remote cache is configured, it checks remotely."),(0,i.kt)("p",null,"If Lerna finds the computation, Lerna retrieves it and replays it. Lerna places the right files in the right folders and prints the terminal output. From the user\u2019s point of view, the command ran the same, just a lot faster."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"cache",src:n(4710).Z,width:"826",height:"522"})),(0,i.kt)("p",null,"If Lerna doesn\u2019t find a corresponding computation hash, Lerna runs the task, and after it completes, it takes the outputs and the terminal logs and stores them locally (and if configured remotely as well). All of this happens transparently, so you don\u2019t have to worry about it."),(0,i.kt)("p",null,"Although conceptually this is fairly straightforward, Lerna optimizes this to make this experience good for you. For instance, Lerna:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Captures stdout and stderr to make sure the replayed output looks the same, including on Windows."),(0,i.kt)("li",{parentName:"ul"},"Minimizes the IO by remembering what files are replayed where."),(0,i.kt)("li",{parentName:"ul"},"Only shows relevant output when processing a large task graph."),(0,i.kt)("li",{parentName:"ul"},"Provides affordances for troubleshooting cache misses. And many other optimizations.")),(0,i.kt)("p",null,"As your workspace grows, the task graph looks more like this:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"cache",src:n(2681).Z,width:"488",height:"467"})),(0,i.kt)("p",null,"All of these optimizations are crucial for making Lerna usable for any non-trivial workspace. Only the minimum amount of work happens. The rest is either left as is or restored from the cache."),(0,i.kt)("h2",{id:"source-code-hash-inputs"},"Source Code Hash Inputs"),(0,i.kt)("p",null,"The result of building/testing an application or a library depends on the source code of that project and all the source codes of all the libraries it depends on (directly or indirectly). It also depends on the configuration files\nlike ",(0,i.kt)("inlineCode",{parentName:"p"},"package.json"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"nx.json"),", and ",(0,i.kt)("inlineCode",{parentName:"p"},"package-lock.json"),". The list of these files isn't arbitrary."),(0,i.kt)("p",null,"Lerna can deduce most of them by analyzing the codebase. If there are exceptions that cannot be inferred automatically, they can be manually listed in the ",(0,i.kt)("inlineCode",{parentName:"p"},"implicitDependencies")," property of ",(0,i.kt)("inlineCode",{parentName:"p"},"nx.json"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "implicitDependencies": {\n    "global-config-file.json": "*"\n  },\n  ...\n}\n')),(0,i.kt)("h2",{id:"runtime-hash-inputs"},"Runtime Hash Inputs"),(0,i.kt)("p",null,"All commands listed in ",(0,i.kt)("inlineCode",{parentName:"p"},"runtimeCacheInputs")," are invoked by Lerna, and the results are included into the computation hash of each task. You can customize them in ",(0,i.kt)("inlineCode",{parentName:"p"},"nx.json"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "tasksRunnerOptions": {\n    "default": {\n      "options": {\n        "cacheableOperations": ["build", "test"],\n        "runtimeCacheInputs": ["node -v", "echo $IMPORTANT_ENV_VAR"]\n      }\n    }\n  }\n}\n')),(0,i.kt)("p",null,"Sometimes the amount of ",(0,i.kt)("em",{parentName:"p"},"runtimeCacheInputs")," can be too overwhelming and difficult to read or parse. In this case, we recommend creating a ",(0,i.kt)("inlineCode",{parentName:"p"},"SHA")," from those inputs. It can be done as follows:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "tasksRunnerOptions": {\n    "default": {\n      "options": {\n        "cacheableOperations": ["build", "test"],\n        "runtimeCacheInputs": [\n          "node -v",\n          "echo $IMPORTANT_ENV_VAR",\n          "echo $LONG_IMPORTANT_ENV_VAR | sha256sum",\n          "cat path/to/my/big-list-of-checksums.txt | sha256sum"\n        ]\n      }\n    }\n  }\n}\n')),(0,i.kt)("h2",{id:"args-hash-inputs"},"Args Hash Inputs"),(0,i.kt)("p",null,"Finally, in addition to Source Code Hash Inputs and Runtime Hash Inputs, Lerna needs to consider the arguments: For example, ",(0,i.kt)("inlineCode",{parentName:"p"},"lerna run build --scope=remixapp")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"lerna run build --scope=remixap -- --flag=true")," produce different results."),(0,i.kt)("p",null,"Note, only the flags passed to the npm scripts itself affect results of the computation. For instance, the following commands are identical from the caching perspective."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"> npx lerna run build --scope=remixapp\n> npx lerna run build --ignore=header,footer\n")),(0,i.kt)("p",null,"In other words, Lerna does not cache what the developer types into the terminal."),(0,i.kt)("p",null,"If you build/test/lint\u2026 multiple projects, each individual build has its own hash value and is either be retrieved from cache or run. This means that from the caching point of view, the following command:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"> npx lerna run build --scope=header,footer\n")),(0,i.kt)("p",null,"is identical to the following two commands:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"> npx lerna run build --scope=header\n> npx lerna run build --scope=footer\n")),(0,i.kt)("h2",{id:"what-is-cached"},"What is Cached"),(0,i.kt)("p",null,"Lerna works on the process level. Regardless of the tools used to build/test/lint/etc.. your project, the results is cached."),(0,i.kt)("p",null,"Lerna sets up hooks to collect stdout/stderr before running the command. All the output is cached and then replayed during a cache hit."),(0,i.kt)("p",null,"Lerna also caches the files generated by a command. The list of files/folders is listed in the ",(0,i.kt)("inlineCode",{parentName:"p"},"outputs")," property of the project's ",(0,i.kt)("inlineCode",{parentName:"p"},"package.json"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "nx": {\n    "targets": {\n      "build": {\n        "outputs": ["./build", "./public/build"]\n      }\n    }\n  }\n}\n')),(0,i.kt)("p",null,"If the ",(0,i.kt)("inlineCode",{parentName:"p"},"outputs")," property is missing, Lerna defaults to caching ",(0,i.kt)("inlineCode",{parentName:"p"},"dist")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"build")," at the root of the project."),(0,i.kt)("h2",{id:"skipping-cache"},"Skipping Cache"),(0,i.kt)("p",null,"Sometimes you want to skip the cache, such as if you are measuring the performance of a command. Use the ",(0,i.kt)("inlineCode",{parentName:"p"},"--skip-nx-cache")," flag to skip checking the computation cache."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"> npx lerna run build --skip-nx-cache\n> npx lerna run test --skip-nx-cache\n")),(0,i.kt)("h2",{id:"customizing-the-cache-location"},"Customizing the Cache Location"),(0,i.kt)("p",null,"The cache is stored in ",(0,i.kt)("inlineCode",{parentName:"p"},"node_modules/.cache/nx")," by default. To change the cache location, update the ",(0,i.kt)("inlineCode",{parentName:"p"},"cacheDirectory")," option for the task runner:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "tasksRunnerOptions": {\n    "default": {\n      "options": {\n        "cacheableOperations": ["build", "test"],\n        "cacheDirectory": "/tmp/mycache"\n      }\n    }\n  }\n}\n')),(0,i.kt)("h2",{id:"local-computation-caching"},"Local Computation Caching"),(0,i.kt)("p",null,"By default, Lerna (via Nx) uses a local computation cache. Nx stores the cached values only for a week, after which they are deleted. To clear the cache run ",(0,i.kt)("inlineCode",{parentName:"p"},"nx reset"),", and Nx creates a new one the next time it tries to access it."),(0,i.kt)("h2",{id:"distributed-computation-caching"},"Distributed Computation Caching"),(0,i.kt)("p",null,"The computation cache provided by Nx can be distributed across multiple machines. You can either build an implementation of the cache or use Nx Cloud. Nx Cloud is an app that provides a fast and zero-config implementation of distributed caching. It's completely free for OSS projects and for most closed-sourced projects (",(0,i.kt)("a",{parentName:"p",href:"https://dev.to/nrwl/more-time-saved-for-free-with-nx-cloud-4a2j"},"read more here"),")."),(0,i.kt)("p",null,"You can connect your workspace to Nx Cloud by running:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"> npx nx connect-to-nx-cloud\n")),(0,i.kt)("p",null,"Learn more about Nx Cloud at ",(0,i.kt)("a",{parentName:"p",href:"https://nx.app"},"https://nx.app"),"."))}d.isMDXComponent=!0},4710:function(e,t,n){t.Z=n.p+"assets/images/cache-c67f30b99d684071f4f042509a39448e.png"},3238:function(e,t,n){t.Z=n.p+"assets/images/lerna-hashing-d90a81e4acb3c97217f66a22f7b8c745.png"},2681:function(e,t,n){t.Z=n.p+"assets/images/task-graph-big-c67fec16567b185ddeed8e7433aad868.png"}}]);